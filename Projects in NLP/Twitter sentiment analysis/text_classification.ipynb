{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from unidecode import unidecode\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_pos=open(\"positive.txt\",\"r\").read()\n",
    "short_neg=open(\"negative.txt\",\"r\").read()\n",
    "\n",
    "short_pos = short_pos.decode('unicode_escape').encode('ascii','ignore')\n",
    "short_neg = short_neg.decode('unicode_escape').encode('ascii','ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]    # for getting all the reviews in the form (review,pos or neg)\n",
    "all_words=[]    # our BAG of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now getting all words in the documents using word tokienizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "allowed_word_types=[\"J\"]  # pos tagging, j means adjactive\n",
    "\n",
    "\n",
    "for p in short_pos.split('\\n'):\n",
    "    documents.append( (p, \"pos\") )\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "            \n",
    "for p in short_neg.split('\\n'):\n",
    "    documents.append( (p, \"neg\") )\n",
    "    words = word_tokenize(p)\n",
    "    pos = nltk.pos_tag(words)\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:  # so that only adjactives will be in our bag of words\n",
    "            all_words.append(w[0].lower())\n",
    "\n",
    "save_documents = open(\"pickled_algo/documents.pickle\",\"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_words=nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = list(all_words.keys())[:5000] #getting top 5000 words\n",
    "\n",
    "\n",
    "##---- pickle\n",
    "\n",
    "save_word_features = open(\"pickled_algo/word_features5k.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()\n",
    "\n",
    "##--------------------------##\n",
    "\n",
    "# find feature function checks if a word that is present in our word_features(BOW for top 5000) is also present in \n",
    "# the document given\n",
    "# after training the final model, for using this module for new document \n",
    "# we will first call this fuction, which will give back word_features.\n",
    "    \n",
    "def find_features(document):\n",
    "    words=word_tokenize(document)\n",
    "    features={}\n",
    "    for w in word_features:\n",
    "        features[w]=(w in words)\n",
    "\n",
    "    return features      ## map of 5000 words,true if present in document and false otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature_Set=pickle.load( open( \"feature.pickle\", \"rb\" ) )\n",
    "\n",
    "feature_Set=[]      ## training and testing data\n",
    "                    ## 5000 word fetures ---> Pos||Neg    \n",
    "                    # 1 2 3 4 5 6 ......... 5000  POS||NEG\n",
    "\n",
    "\n",
    "for rev,category in documents: # documents contain reviews by line and their tags(pos or neg)\n",
    "    feature_Set.append((find_features(rev),category))\n",
    "\n",
    "\n",
    "random.shuffle(feature_Set)\n",
    "\n",
    "## pickle\n",
    "save_feature=open(\"pickled_algo/feature.pickle\",\"wb\")\n",
    "pickle.dump(feature_Set,save_feature)\n",
    "save_feature.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set=feature_Set[10000:]\n",
    "test_set=feature_Set[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier=nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "#pickle\n",
    "save_feature=open(\"pickled_algo/originalnb.pickle\",\"wb\")\n",
    "pickle.dump(classifier,save_feature)\n",
    "save_feature.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Naive Bayes  Algo accuracy percentage :', 60.209999999999994)\n"
     ]
    }
   ],
   "source": [
    "print (\"Naive Bayes  Algo accuracy percentage :\",(nltk.classify.accuracy(classifier,test_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                    well = True              pos : neg    =      7.9 : 1.0\n",
      "                    best = True              pos : neg    =      4.8 : 1.0\n",
      "                     bad = True              neg : pos    =      4.1 : 1.0\n",
      "                romantic = True              pos : neg    =      3.8 : 1.0\n",
      "               cinematic = True              pos : neg    =      3.8 : 1.0\n",
      "                    dull = True              neg : pos    =      3.6 : 1.0\n",
      "                   debut = True              pos : neg    =      3.1 : 1.0\n",
      "                   sharp = True              pos : neg    =      3.1 : 1.0\n",
      "                  female = True              neg : pos    =      2.9 : 1.0\n",
      "                      de = True              neg : pos    =      2.9 : 1.0\n",
      "                 earnest = True              neg : pos    =      2.9 : 1.0\n",
      "                   quite = True              neg : pos    =      2.9 : 1.0\n",
      "                     big = True              pos : neg    =      2.7 : 1.0\n",
      "                    plot = True              neg : pos    =      2.6 : 1.0\n",
      "                    less = True              neg : pos    =      2.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using sklearn\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC,LinearSVC,NuSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb={}\n",
    "linear={}\n",
    "svm={}\n",
    "clas={1:[nb,2],2:[linear,2],3:[svm,2]}\n",
    "nb[1]=(MultinomialNB(),\"MultinomialNB\")\n",
    "nb[2]=(BernoulliNB(),\"BernoulliNB\")\n",
    "linear[1]=(LogisticRegression(),\"LogisticRegression\")\n",
    "linear[2]=(SGDClassifier(),\"SGDClassifier\")\n",
    "svm[1]=(LinearSVC(),\"LinearSVC\")\n",
    "svm[2]=(NuSVC(),\"NuSVC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('classifier MultinomialNB ', 60.309999999999995)\n",
      "('classifier BernoulliNB ', 60.17)\n",
      "('classifier LogisticRegression ', 58.64)\n",
      "('classifier SGDClassifier ', 58.699999999999996)\n",
      "('classifier LinearSVC ', 59.25)\n",
      "('classifier NuSVC ', 57.49999999999999)\n"
     ]
    }
   ],
   "source": [
    "fin_classi=[]\n",
    "for i in range(1,4):\n",
    "    for j in range(1,clas[i][1]+1):\n",
    "        dic=clas[i][0]\n",
    "        classif=SklearnClassifier(dic[j][0])\n",
    "        \n",
    "        classif.train(train_set)\n",
    "        \n",
    "        #pickle\n",
    "        k=dic[j][1]+'.pickle'\n",
    "        save_feature=open(k,\"wb\")\n",
    "        pickle.dump(feature_Set,save_feature)\n",
    "        save_feature.close()\n",
    "        ###\n",
    "        \n",
    "        fin_classi.append(classif)\n",
    "        print (\"classifier {} \".format(dic[j][1]),(nltk.classify.accuracy(classif,test_set))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function for voting between the classifier to get best result  also to get confidence level.\n",
    "# should be used after training  individual models\n",
    "\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "class Vclassify(ClassifierI):\n",
    "    def __init__(self,classifiers):\n",
    "        self._classifiers=classifiers\n",
    "    def classify(self,features):\n",
    "        votes=[]\n",
    "        for c in self._classifiers:\n",
    "            \n",
    "            v=c.classify(features)\n",
    "            votes.append(v)\n",
    "      \n",
    "        return mode(votes)\n",
    "    def confidence(self,features):\n",
    "        votes=[]\n",
    "        for c in self._classifiers:\n",
    "            v=c.classify(features)\n",
    "            votes.append(v)\n",
    "        choice_votes=votes.count(mode(votes))\n",
    "        conf=choice_votes/len(votes)\n",
    "        return conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('classifier', 59.870000000000005)\n"
     ]
    }
   ],
   "source": [
    "fin_classi.append(classifier) #should have odd number of classifiers,so added our original NB classifier from nltk\n",
    "\n",
    "votes_classifier=Vclassify(fin_classi)\n",
    "print (\"classifier\",(nltk.classify.accuracy(votes_classifier,test_set))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}